{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pipelines","text":"<p>This repo contains various analysis pipelines for the lab. Here are the basic rules:</p> <ul> <li>each folder includes pipelines for a particular analysis - data type combination</li> <li>pipelines are nextflow workflows</li> <li>each pipeline comes with a list of conda environment files that manage the required software   or a docker image that packages the required software</li> </ul>"},{"location":"#data-layout","title":"Data layout","text":"<p>Pipelines will usually operate from a top level project directory structured in the following way:</p> <pre><code>[project root]\n\u251c\u2500 [pipeline].nf              # optional, see setup\n\u251c\u2500 data                       # anything that is not code\n\u2502  \u251c\u2500 raw\n\u2502  \u2502  \u251c\u2500 sample1_R1.fastq.gz\n\u2502  \u2502  \u251c\u2500 sample1_R2.fastq.gz\n\u2502  \u2502  \u2514\u2500 ...\n\u2502  \u2514\u2500 ...\n\u251c\u2500 figures\n\u2502  \u251c\u2500 fig1.png\n\u2502  \u2514\u2500 ...\n\u2514\u2500 refs                       # often a symbolic link\n   \u251c\u2500 eggnog\n   \u2514\u2500 kraken2\n</code></pre> <p>The initial raw data lives in <code>data/raw</code> and all analysis artifacts should be written into <code>data/</code> as well. Figures go into <code>figures/</code>.</p>"},{"location":"16S/","title":"16S amplicon sequencing workflow","text":""},{"location":"16S/#overview","title":"Overview","text":""},{"location":"16S/#feasible-data","title":"Feasible data:","text":"<ul> <li>single-end or paired-end 16S amplicon sequencing data</li> <li>decent depth (&gt;10K reads per sample)</li> </ul>"},{"location":"16S/#steps","title":"Steps:","text":"<ol> <li>Automatic detection of Illumina read files and management of multiple runs</li> <li>Trimming, filtering and quality metrics (base qualities, entropy, lengths) with DADA2 and mbtools</li> <li>Denoising using DADA2</li> <li>Taxonomy assignment using DADA2 and GTDB 220</li> <li>Alignment of ASVs with DECIPHER and building of a phylogenetic tree from core alignment (FastTree).</li> <li>Creation of tabular and phyloseq output</li> </ol>"},{"location":"16S/#setup","title":"Setup","text":""},{"location":"16S/#option-1-everything-in-a-conda-env","title":"Option 1: Everything in a conda env","text":"<pre><code>mamba env create -f conda.yml\nconda activate 16S\nRscript -e \"remotes::install_github('dienerlab/miso')\"\n</code></pre> <p>This can be activated with <code>conda activate 16S</code>.</p>"},{"location":"16S/#options-2-local-r-everything-else-in-conda","title":"Options 2: Local R, everything else in conda","text":"<p>This requires an installation of R on all nodes running the pipeline.</p> <p>Tip</p> <p>Only use this option if you are already using R for lots of other stuff. This might also use an older DADA2 version depending on your local R version.</p> <pre><code>mamba env create -f conda-no-r.yml\nRscript -e \"install.packages('BiocManager')\"\nRscript -e \"remotes::install_github('dienerlab/miso')\"\n</code></pre> <p>This can be activated with <code>conda activate 16S-no-r</code>.</p>"},{"location":"16S/#download-taxonomy-databases","title":"Download taxonomy databases","text":"<p><code>cd</code> into your work directory, then</p> <pre><code>mkdir -p refs\nwget https://zenodo.org/records/10403693/files/GTDB_bac120_arc53_ssu_r220_genus.fa.gz?download=1 -O refs/GTDB_bac120_arc53_ssu_r220_genus.fa.gz\nwget https://zenodo.org/records/10403693/files/GTDB_bac120_arc53_ssu_r220_species.fa.gz?download=1 -O refs/GTDB_bac120_arc53_ssu_r220_species.fa.gz\n</code></pre>"},{"location":"16S/#workflow-parameters","title":"Workflow parameters","text":"<pre><code>~~~ Diener Lab 16S Workflow ~~~\n\nUsage:\nA run using all,default parameters can be started with:\n&gt; nextflow run main.nf -resume\n\nGeneral options:\n    --data_dir [str]              The main data directory for the analysis (must contain `raw`).\n    --read_length [int]           The length of the reads.\n    --forward-only [bool]         Run analysis only on forward reads.\n    --threads [int]               The maximum number of threads a single process can use.\n                                This is not the same as the maximum number of total threads used.\n    --manifest [str]              A manifest file listing the files to be processed. Should be a CSV file with\n                                columns \"id\", \"forward\", \"reverse\" (optional), and \"run\" (optional). Listing the\n                                sample IDs and read files. If samples were sequenced in different runs indicate\n                                this with the run column.\n    --pattern [str]               The file pattern for the FASTQ files. Options are illumina, sra, and simple.\n                                Only used if no manuscript was provided.\n\nReference DBs:\n    --taxa_db [str]               Path to the default taxonomy database.\n    --species_db [str]            Path to species database to perform exact matching to ASVs.\n\nQuality filter:\n    --trim_left [int]             How many bases to trim from the 5' end of each read.\n    --trunc_forward [int]         Where to truncate forward reads. Default length - 5\n    --trunc_reverse [int]         Where to truncate reverse reads. Default length - 20.\n    --maxEE [int]                 Maximum number of expected errors per read.\n\nDenoising:\n    --min_overlap [int]           Minimum overlap between reverse and forward ASVs to merge them.\n    --merge [bool]                Whether to merge several runs into a single output.\n</code></pre> <p>Most likely run using:</p> <pre><code>DLP=/path/to/pipelines\nnextflow run $DLP/16S -resume -with-conda [PATH/TO/CONDA]/envs/16S[-no-r]\n</code></pre> <p>If you have only one run put all raw FASTQ files under <code>raw</code>.</p>"},{"location":"release-notes/","title":"Release Notes","text":"<p>This lists general release notes for the pipelines.</p>"},{"location":"release-notes/#version-100","title":"version 1.0.0","text":"<p>Major Changes</p> <p>Revamped the documentation and actions.</p> <p>Minor Changes</p>"},{"location":"setup/","title":"Setup","text":"<p>We first recommend to clone the repository.</p> gitGitHub CLI <pre><code>git clone https://github.com/dienerlab/pipelines\n</code></pre> <pre><code>gh repo clone dienerlab/pipelines\n</code></pre> <p>It's also a good idea to install nextflow into your conda base environment.</p> <pre><code>conda install -c conda-forge -c bioconda nextflow\n</code></pre>"},{"location":"setup/#install-environments","title":"Install environments","text":"<p>Most of the pipelines contain a conda environment file that can be used. For instance, to set up the metagenomics environment:</p> <pre><code>cd /path/to/cloned/repository\nconda env create -f metagenomics/conda.yml\n</code></pre> <p>No additional setup is required for pipelines using docker or apptainer/singularity.</p>"},{"location":"setup/#configure-nextflow","title":"Configure Nextflow","text":"<p>If you are part of our lab pleae use the configuration from our wiki.</p> <p>Otherwise you can use the following basic configuration which sets up profiles for local execution or SLURM:</p> nextflow.config<pre><code>profiles {\n    standard {\n        executor {\n            name = \"local\"\n            cpus = 12\n            memory = \"128 GB\"\n        }\n    }\n\n    slurm {\n        executor {\n            name = \"slurm\"\n            queueSize = 40\n            pollInterval = '2 min'\n            dumpInterval = '3 min'\n            queueStatInterval = '5 min'\n            exitReadTimeout = '10 min'\n            killBatchSize = 50\n            submitRateLimit = '40 min'\n        }\n        process.queue = \"cpu\"\n\n    }\n\n    tower {\n        workspaceId = \"&lt;my-id&gt;\"\n        accessToken = \"&lt;my-token&gt;\"\n        enabled = false\n    }\n}\n\nsingularity.runOptions = \"-B $TMPDIR\"\n</code></pre> <p>Make sure to make any adjustments beforehand. For instance, for SLURM you probably want to use the correct name for the queue in your HPC system.If you use Seqera Cloud (formerly Tower) also add in your token and woskspace ID and set it to enabled.</p> <p>Either copy this file into your project directory or create the file <code>~/.nextflow/config</code> in your home directory and copy the contents there.</p> <pre><code>mkdir ~/.nextflow\n</code></pre> <p>After that edit and copy the config:</p> <pre><code>cp /path/to/pipelines/nextlow.config ~/.nextflow/config\n</code></pre> <p>You can switch between the profiles by using the Nextflow <code>-profile</code> flag.</p> <pre><code>nextflow run [PIPELINE] -profile slurm ...\n</code></pre> <p>Note</p> <p>For the lab config the default is already using SLURM. You only need to specify the profile if you want to force local execution:</p> <pre><code>nextflow run [PIPELINE] -profile local ...\n</code></pre>"},{"location":"setup/#run-the-pipelines","title":"Run the pipelines","text":"<p>There are generally two options to run the pipelines:</p> <ol> <li>Directly call the pipeline from the cloned directory</li> <li>Create a symlink to the specific <code>[pipeline].nf</code> file</li> </ol> directlink <pre><code>export DLP=/path/to/cloned/repository\nnextflow run $DLP/metagenomics ...\n</code></pre> <pre><code>ln -s /path/to/cloned/repository/metagenomics/main.nf\nnextflow run main.nf ...\n</code></pre> <p>If the pipeline is using a conda environment it should be run using the <code>-with-conda</code> flag indicating the directory where the environment was created, usually one in <code>~/miniforge3/envs</code>. For the docker based ones usually it is run using apptainer/singularity.</p> <p>Please see the individual pipelines for instructions.</p> <p>For the metagenomics pipeline this looks like:</p> <pre><code>nexflow run $DLP/metagenomics/main.nf -resume -with-conda=$HOME/miniforge3/envs/metagenomics ...\n</code></pre>"},{"location":"concepts/batching/","title":"Strategies and concept for the workflows","text":""},{"location":"concepts/batching/#local-memory-cashing-of-kraken-databases","title":"Local memory cashing of Kraken databases","text":"<p>For large Kraken2 databases reading the database into memory will quickly becoming the largest time sink when classifying. When running many Kraken2 jobs in parallel this can be circumvented by using shared memory where the first Kraken2 job will load the database into a memory segment readable by all Kraken2 jobs. This will often show up as \"cached\" in your memory overviews. As slong as this cached segment is not freed by the OS, databases will be loaded instantaneously by all future Kraken2 jobs. This will lead immense performance gains.</p> <p></p> <p>This will usually not work for HPC systems as the jobs will be distributed across different machines with their own memory and RAM. This is why the Kraken2 steps here allow batching of Kraken2 jobs into groups that are guaranteed to run on the same machine and thus will gain the performance benefit. The caveat here is that all jobs in a batch will fail together. So if one of the Kraken2 jobs crashes all output from the batch is lost. In practice, one will aim to make the batch as large as possible while still retaining some resilience for failing jobs.</p> <p>Note</p> <p>For single machine setups (local runs) the optimal batch size is always 1.</p>"},{"location":"mgx/basic/","title":"Metagenomics","text":""},{"location":"mgx/basic/#overview","title":"Overview","text":"<p>The metagenomics workflow(s) consist of a basic gene cluster-based workflow followed by add-ons for binning an replication rate inference.</p> <p>The basic workflow is a protein/gene-centric workflow. It will quantify taxon abundances as well as abundances for protein-coding orthologous gene clusters (groups of genes with putatively similar function across organisms). It is feasible to answer the following questions:</p> <ul> <li>Are the taxa whose abundances are related to a phenotype?</li> <li>Are there proteins whose genes are more prevalent/abundant in a specific phenotype?</li> <li>What functional potential is present in particular metagenomes?</li> </ul>"},{"location":"mgx/basic/#feasible-data","title":"Feasible data:","text":"<ul> <li>paired or single end metagenomic shotgun sequencing</li> <li>any depth (there is no separate shallow pipeline anymore)</li> </ul> <p>Warning</p> <p>The basic workflow currently does not work with Nanopore data due to the salmon step. We will add compatibility soon. You can still run all the previous steps which are fully compatible.</p>"},{"location":"mgx/basic/#steps","title":"Steps:","text":"<ol> <li>Adapter and quality trimming with fastp (1)</li> <li>Read annotation with Kraken2 with some custom HPC optimization</li> <li>Taxon counting using Bracken</li> <li>Assembly with MegaHit</li> <li>De novo gene prediction with prodigal</li> <li>Clustering of all genes on protein identity using mmseqs2 linclust</li> <li>Pufferfish mapping index creation (needed for next step)</li> <li>Gene quantification (mapping + counting) with salmon</li> <li>Protein annotation using the EGGNoG mapper</li> </ol> <ol> <li>quality reports in HTML and JSON are provided for each file</li> </ol>"},{"location":"mgx/basic/#setup","title":"Setup","text":"<p>For the basic workflow set up the <code>metagenomics</code> environment.</p> <pre><code>conda env create -f pipelines/metagenomics/conda.yml\n</code></pre>"},{"location":"mgx/basic/#workflow-options","title":"Workflow options","text":"<pre><code>~~~ Diener Lab Metagenomics Workflow ~~~\n\nUsage:\nA run using all,default parameters can be started with:\n&gt; nextflow run main.nf --resume\n\nAn exampl erun could look like:\n&gt; nextflow run main.nf -with-conda /my/envs/metagenomics -resume \\\n                        --data_dir=./data --single_end=false --refs=/my/references \\\n                        --read_length=150\n\nGeneral options:\n  --data_dir [str]              The main data directory for the analysis (must contain `raw`).\n  --read_length [str]           The length of the reads.\n  --single_end [bool]           Specifies that the input is single-end reads.\n  --threads [int]               The maximum number of threads a single process can use.\n                                This is not the same as the maximum number of total threads used.\nReference DBs:\n  --refs [str]                  Folder in which to find references DBs.\n  --eggnogg_refs [str]          Where to find EGGNOG references. Defaults to &lt;refs&gt;/eggnog.\n  --kraken2_db [str]            Where to find the Kraken2 reference. Defaults to &lt;refs&gt;/kraken2_default.\n  --kraken2_mem [str]           The maximum amount of memory for Kraken2. If not set will choose this automatically\n                                based on the database size. Thus, only use to limit Kraken2 to less memory.\nQuality filter:\n  --trim_front [str]            How many bases to trim from the 5' end of each read.\n  --min_length [str]            Minimum accepted length for a read.\n  --quality_threshold [str]     Smallest acceptable average quality.\n  --threshold [str]             Smallest abundance threshold used by Kraken.\n\nAssembly:\n  --contig_length [int]         Minimum length of a contig.\n  --identity [double]           Minimum average nucleotide identity.\n  --overlap [double]            Minimum required overlap between contigs.\n\nTaxonomic classification:\n  --batchsize [int]             The batch size for Kraken2 jobs. See documentation\n                                for more info. Should be 1 on single machine setups\n                                and much larger than one on HPC setups.\n  --kraken2_mem [int]           Maximum memory in GB to use for Kraken2. If not set\n                                this will be determined automatically from the database.\n                                So, only set this if you want to overwrite the automatic\n                                detection.\n</code></pre> <p>Additional workflows can be run after the basic workflow has finished.</p> <p>Tip</p> <p>On HPC systems we recommend to set the <code>--batchsize</code> option to something close to the number of samples unless you either have lots (&gt;1000) or problematic samples.</p> <p>See concepts for more info on batches.</p>"},{"location":"mgx/binning/","title":"Binning","text":"<p>If you want to run metagenomic binning as well also set up the <code>binchecks</code> environment.</p> <pre><code>conda env create binchecks.yml\n</code></pre>"},{"location":"mgx/binning/#binning-workflow","title":"Binning workflow","text":"<p>Definition: <code>binning.nf</code></p>"},{"location":"mgx/binning/#steps","title":"Steps:","text":"<ol> <li>metagenomic binning with Metabat2</li> <li>Quality checks with checkM2</li> <li>taxonomy assignment using GTDB-TK</li> <li>Dereplication with dRep using the checkM2 metrics and skANI</li> </ol> <p>If no manifest is provided this will use the contigs and preprocessed reads from the basic workflow. Otherwise, you can provide a CSV manifest with the columns \"id\", \"contigs\", \"forward\" and (optionally) \"reverse\" to specify the sample id, contigs, and read files.</p> <p>[!NOTE] When specifying a manifest please note that the file paths are assumed to be relative to the current project directory.</p>"},{"location":"mgx/binning/#options","title":"Options:","text":"<pre><code>~~~ Diener Lab Binning workflow ~~~\n\nUsage:\nA run using all,default parameters can be started with:\n&gt; nextflow run binning.nf --resume\n\nAn example run would look like\n&gt; nextflow run binning.nf -resume \\\n                          --conda_path $HOME/miniconda3/envs \\\n                          --min_contig_length 5000 --ani 0.95\n\nGeneral options:\n  --data_dir [str]              The main data directory for the analysis (must contain `raw`).\n  --single_end [bool]           Whether the data is single-end sequencing data.\n  --preset [str]                What sequencing technology was used. Can be \"illumina\" or\n                                \"nanopore\".\n  ---manifest [path]            Location of a manifest containing paths to contigs and reads.\n                                This location is relative to the project directory.\n\nBinning options:\n  --min_contig_length [int]     Minimum length of the contigs to include them.\n  --min_bin_size [int]          Miinimum length of the binned genome.\n\nDereplication options:\n  --ani [float]                 On what ANI to dereplicate the MAGs. The default is\n                                adequate for strain level analyses.\n\nReference DBs:\n  --checkm [path]                Location of the checkM2 uniref DB.\n  --gtdb [path]                  Location of the GTDB-TK database.\n</code></pre>"},{"location":"rep/mgx/","title":"metagenomics","text":""},{"location":"rep/mgx/#replication-rates","title":"Replication rates","text":"<p>Definition: replication.nf</p> <ol> <li>Alignment to ~3K high quality assemblies from the gut microbiome with bowtie2</li> <li>extraction of coverage maps</li> <li>Quanitifaction of peak-to-trough ratios (PTRs) with coptr</li> </ol> <p>This will use the preprocessed data from the basic workflow. The output will be a single CSV file called <code>data/annotated_rates.csv</code> that contains the replication rates and species annotations.</p>"},{"location":"rep/mgx/#options","title":"Options:","text":"<pre><code>~~~ Diener Lab Replication Rate workflow ~~~\n\nUsage:\nA run using all,default parameters can be started with:\n&gt; nextflow run replication.nf --resume\n\nA run with all parametrs set would look like:\n&gt; nextflow run main.nf --data_dir=./data --single_end true --threads 12 --min_reads 2500 --IGG /refs/IGG\n\nGeneral options:\n  --data_dir [str]              The main data directory for the analysis (must contain `raw`).\n  --single_end [bool]           Whether the data is single-end sequencing data.\n\nCOPTR options:\n  --min_reads [int]             Minimum number of reads for a genome to calculate PTRs.\n  --IGG [path]                  Location of the IGG bowtie2 reference.\n</code></pre>"},{"location":"tips/meta-per-file/main/","title":"Complex meta analysis","text":"<p>Coming soon</p>"},{"location":"tips/meta-per-study/main/","title":"Simple meta analysis","text":"<p>This is a strategy for a simple meta-analysis nextflow workflow. Here \"simple\" means that all you individual processing steps are already expected to run on all the data in a single study. This would be the case for Qiime2 analyses for instance because each Qiime2 always operates on all data from a study.</p> <p>Here we first have a main table that lists the studies with the required parameters/settings for each study.</p> main table<pre><code>id,layout,forward_primer,reverse_primer,trunc_f,trunc_r,location\nstudy 1,paired,ACG,TGC,220,200,study_1\nstudy 2,single,ACT,GCT,220,0,study_2\n</code></pre> <p>The nextflow pipeline then reads the CSV and passed the study information as value in all the steps. The initial channel has one entry for each study.</p> pipeline example<pre><code>#!/usr/bin/env nextflow\n\nparams.data = \"${launchDir}\"\nparams.studies = \"${params.data}/studies.csv\"\nparams.visualize = true\n\nworkflow {\n    // Read the studies table\n    studies = Channel.fromPath(params.studies)\n        .splitCsv(header: true, sep: \",\")\n\n    // Do some work on the studies\n\n    studies | import_data | step1\n\n    // Visualize if desired\n    if (params.visualize) {\n        visualize(import_data.out)\n    }\n\n}\n\nprocess import_data {\n    publishDir \"${params.data}/imports\", mode: 'copy', overwrite: true\n\n    cpus 4\n    memory \"8GB\"\n    time \"2h\"\n\n    input:\n    val(study)\n\n    output:\n    tuple val(study), path(\"*.txt\")\n\n    script:\n    if (study.layout == \"paired\") {\n        \"\"\"\n        echo \"importing from ${params.data}/${study.location}/manifest.tsv in paired-end layout.\" &gt; '${study.id}.txt'\n        \"\"\"\n    } else if (study.layout == \"single\") {\n        \"\"\"\n        echo \"importing from ${params.data}/${study.location}/manifest.tsv in single-end layout.\" &gt; '${study.id}.txt'\n        \"\"\"\n    } else {\n        error \"Invalid libray layout specified. Must be 'paired' or 'single' :(\"\n    }\n}\n\nprocess step1 {\n    publishDir \"${params.data}/step1\", mode: 'copy', overwrite: true\n\n    cpus 1\n    memory \"2GB\"\n    time \"1h\"\n\n    input:\n    tuple val(study), path(imported)\n\n    output:\n    tuple val(study), path(\"*.result\")\n\n    script:\n    \"\"\"\n    echo \"processed ${study.id} data with truncations of ${study.trunc_f},${study.trunc_r}\" &gt; '${study.id}.result'\n    \"\"\"\n}\n\nprocess visualize {\n    publishDir \"${params.data}/viz\", mode: 'copy', overwrite: true\n\n    cpus 1\n    memory \"2GB\"\n    time \"1h\"\n\n    input:\n    tuple val(study), path(imported)\n\n    output:\n    tuple val(study), path(\"${study.id}.viz\")\n\n    script:\n    \"\"\"\n    echo \"visualized ${study.id} import\" &gt; '${study.id}.viz'\n    \"\"\"\n}\n</code></pre> <p>Note</p> <p>The individuals processng scripts don't make much sense here. Those just serve as an example how to inject the study parameters.</p> <p>Running this will then distribute the row for each study.</p> <pre><code>$ nextflow run main.nf\n\n N E X T F L O W   ~  version 25.03.1-edge\n\nLaunching `main.nf` [stoic_murdock] DSL2 - revision: 0e2f97c09b\n\nexecutor &gt;  local (6)\n[ec/8110b0] process &gt; import_data (1) [100%] 2 of 2 \u2714\n[12/497c3c] process &gt; step1 (1)       [100%] 2 of 2 \u2714\n[1e/c9f3fe] process &gt; visualize (2)   [100%] 2 of 2 \u2714\n</code></pre> <p>This also shows an example how to globally disable a part of the pipeline (visualization) while still retaining the cache.</p> <pre><code>$ nextflow run main.nf -resume --visualize false\n\n N E X T F L O W   ~  version 25.03.1-edge\n\nLaunching `main.nf` [intergalactic_goldwasser] DSL2 - revision: 0e2f97c09b\n\n[f9/53dc3a] process &gt; import_data (2) [100%] 2 of 2, cached: 2 \u2714\n[04/179b27] process &gt; step1 (1)       [100%] 2 of 2, cached: 2 \u2714\n</code></pre> <p>You can see the injection of the library layout and the truncation parameters from the main table.</p> <pre><code>cat imports/*.txt\n</code></pre> <pre><code>importing from /home/cdiener/code/pipelines/docs/tips/meta-per-study/study_1/manifest.tsv in paired-end layout.\nimporting from /home/cdiener/code/pipelines/docs/tips/meta-per-study/study_2/manifest.tsv in single-end layout.\n</code></pre> <pre><code>cat steps/*.result\n</code></pre> <pre><code>processed study 1 data with truncations of 220,200\nprocessed study 2 data with truncations of 220,0\n</code></pre>"}]}